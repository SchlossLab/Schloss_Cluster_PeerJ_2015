---
title: "Application of database-independent methods to assess the quality of OTU picking methods"
bibliography: references.bib
output:
  pdf_document:
    includes:
      in_header: header.tex
csl: msystems.csl
fontsize: 11pt
geometry: margin=1.0in
---




\begin{center}
\vspace{25mm}
Patrick D. Schloss${^\dagger}$

\vspace{30mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI
\end{center}


\newpage
\linenumbers

## Abstract
150 words


\newpage

The ability to assign 16S rRNA gene sequences to operational taxonomic units (OTUs) allows microbial ecologists to overcome the inconsistencies and biases within bacterial taxonomy and provides a strategy for clustering similar sequences that do not have representatives in a reference database. These sequences are clustered into OTUs based on their distance from (or similarity to) each other. Numerous algorithms for solving this seemingly simple problem have blossomed in recent years and were recently the subject of benchmarking studies performed by Westcott and myself [-@Schloss2011Assessing; -@Westcott2015], He et al. [-@He2015], and Kopylova et al [-@Kopylova2016]. These studies provide a thorough review of the sequencing clustering landscape, which can be divided into three general approaches: (i) *de novo* clustering where sequences are clustered without first mapping sequences to a reference database, (ii) closed-reference clustering where sequences are clustered based on the references that the sequences map to, and (iii) open reference clustering where sequences that do not map adequately to the reference are then clustered using a *de novo* approach. My studies have highlighted a persistent problem in the development of clustering algorithms, which is assessing the quality of the clustering assignments.

The most recent analysis of Kopylova et al [-@Kopylova2016] repeats many of the benchmarking strategies employed by previous researchers. First, they and others have compared the time and memory required to cluster sequences in a dataset [@Sun2009; @Cai2011; @Rideout2014; @Mah2014]. These are valid parameters to assess when judging a clustering method, but have little to say about the quality of the OTU assignments. In the extreme, we could easily develop a "toy" a clustering algorithm could randomly assign sequences to a predetermined number of OTUs and be efficient, but also poorly reflect the genetic diversity within the community. Second, they and others have compared the number of OTUs generated by various methods for a common dataset [@Edgar2013; @Kopylova2016]. Again, a toy clustering algorithm could cluster to a target number of OTUs, but the clusterings would likely be meaningless. Third, because many of the methods are sensitive to the initial order of the sequences, a metric of OTU stability has been proposed as a way to assess algorithms [@He2015]. Although it is important that the methods generate reproducible OTU assignments, this approach ignores the possability that the variation in assignments may be equally robust or that the assignemnts by a highly reproducible algorithm may be quite poor. Fourth, the method that Kopylova et al. [-@Kopylova2016] relied upon the most was to cluster sequences from simulated data or data from synthetic communities of cultured organisms and quantify how well the OTU assignments matched the organisms' taxonomy [@Edgar2013; @Mah2014; @Mah2015; @Barriuso2011; @Bonder2012; @Chen2013; @Huse2010; @May2014; @Cai2011; @Sun2011; @White2010; @AlGhalith2016]. This commonly used approach is flawed because bacterial taxonomy often reflects those inconsistencies and biases within bacterial taxonomy that OTU-based methods strive to overcome. Furthermore, this benchmarking strategy can only be applied when the actual taxonomy of the organisms are known and so it is unclear how the methods scale to sequences from the novel organisms we are likely to encounter in deep sequencing surveys. In a final approach, Kopylova and others have assessed the quality of clustering based on their ability to generate the same OTUs generated by other methods [@Schmidt2014Limits; @Rideout2014]. Unfortunately, without the ability to ground truth any method, such comparisons are tenuous. Westcott and I have proposed an unbiased and objective method for assessing the quality of OTU assignments that can be applied to any collection of sequences [@Schloss2011Assessing; @Westcott2015].

Our approach uses the observed dissimilarity between pairs of sequences and information about whether sequences were clustered together to quantify how well similar sequences are clustered together and dissimilar sequences are clustered apart. To quantify the correlation between the observed and expected OTU assignments, we synthesize the relationship between OTU assignments and the distances between sequences using the Matthew's correlation coefficient [MCC; @Matthews1975]. In the most recent application of this approach [@Westcott2015], we found that closed-reference clustering algorithms could be sensitive to the order of the sequences in the reference database and frequently clustered sequences together that were more than 3% different from each other. We also discussed that because open-reference clustering was dependent on closed-reference clustering it had sensitivity to the order of the database. Furthermore, we described how it had a nebulous threshold for OTUs since sequences are clustered based on a radius of 3% under the closed-reference phase and a diameter of 3% under the open reference phase. Finally, we showed that *de novo* clustering algorithms generated the most robust OTU assignments and confirmed our previous analysis that the average neighbor algorithm consistently performed the best [@Schloss2011Assessing]. Given the observation that the best algorithm may vary by dataset we concluded that researchers should quantify the MCC for several *do novo* algorithms before selecting an algorithm.

To revisit these results, I have expanded the analysis to evaluate three hierarchical and seven greedy *de novo* algorithms, one open-reference clustering algorithm, and four closed-reference algorithms (Figure 1). To test these approaches I applied each of them to datasets from soil [@Roesch2007], mouse feces [@Schloss2012Stabilization], and two simulated datasets. The simulated communities were generated by randomly selecting 10,000 16S rRNA sequences that were unique within the V4 region from the SILVA non-redundant database [@Kopylova2016; @Pruesse2007]. Next, an even community was generated by specifying that each sequence had a frequency of 100 reads and a staggered community was generated by specifying that the abundance of each sequence was a randomly drawn a uniform distribution between 1 and 200. A reproducible version of this manuscript and analysis has been added to the repository available at https://github.com/SchlossLab/Schloss_Cluster_PeerJ_2015.



I replicated the benchmarking approach that I have used previously to assess the ability of an algorithm to correctly group sequences that are similar to each other and split sequences that are dissimilar to each other using the MCC [@Westcott2015; @Schloss2011Assessing]. When I compared the MCC values calculated using the ten *de novo* algorithms with the four datasets, the average neighbor algorithm reliably performed as well or better than the other methods (Figure 1). The MCC values for the VSEARCH (AGC: 0.76 and DGC: 0.78) and USEARCH-based (AGC: 0.76 and DGC: 0.77) algorithms, Sumaclust (0.76), and average neighbor (0.76) were similarly high for the murine dataset. For each of the other datasets, the MCC value for the average neighbor algorithm was at least 5% higher than the next best method. Swarm does not use a traditional distance-based criteria to cluster sequences into OTUs and instead looks for natural subnetworks in the data. When I used the distance threshold that gave the best MCC value for the Swarm data, the MCC values were generally not as high as they were using the average neighbor algorithm. The one exception was for the soil dataset. Among the reference-based methods, all of the MCC values suffer because when sequences that are at least 97% similar to a reference are pooled, the sequences within an OTU could be as much as 6% different from each other. The effect of this is observed in the MCC values that were calculated for the OTUs assigned by these methods generally being lower than those observed using the *de novo* approaches (Figure 1). It is also important to note that the MCC values are somewhat inflated because sequences were not clustered into OTUs if there was not a reference sequence that was more than 97% similar to the sequence. Given the consistent quality of the clusterings formed by the average neighbor algorithm, these results confirm the conclusion from the previous analysis that researchers should use the average neighbor algorithm or calculate MCC values for several methods and use the clustering that gives the best MCC value.



Next, I investigated the ability of the reference-based methods to properly assign sequences to OTUs. The full-length 16S rRNA gene sequences in the default reference taxonomy that accompanies QIIME are not more than 97% similar to each other, but within the V4 region many of the sequences were more similar to each other and even identical to each other. As a result, we previously found that For USEARCH and VSEARCH there was a dependence between the ordering of sequences in the reference database and the OTU assignments. To explore this further, we analyzed the 32,106 unique sequences from the murine dataset with randomized databases. VSEARCH always found matches for 27,737 murine sequences, the reference matched to those sequences differed between randomizations. For USEARCH there were between 28007 and 28111 matches depending on the order of the reference. In the updated analysis we found that SortMeRNA resulted in between 23912 and 28464 matches. Using NINJA-OPS with different orderings of the reference sequences generated the same 28,499 matches. These results point to an additional problem with closed-reference clustering, which is the inability for the method to assign sequences to OTUs when a similar reference sequence does not exist in the database. For the well-characterized murine microbiota, NINJA-OPS did the best by finding relatives for 88.77% of the sequences. As indicated by the variation in the number of sequences that matched a reference sequence, these methods varied in their sensitivity and specificity to find the best reference sequence. Of the closed-reference methods, NINJA-OPS had the best sensitivity ( 99.74%) and specificity (79.71%) while SortMeRNA had the worst sensitivity (95.69%) and VSEARCH had the worst specificity (60.31%).  Reference-based clustering algorithms are much faster than *de novo* approaches, but do not generate OTUs that are as robust.

Although the goal of Kopylova et al. [-@Kopylova2016] was to compare various clustering algorithms, they also studied these algorithms in the broader context of raw sequence processing, screening for chimeras, and removal of singletons. Each of these are critical decisions in a comprehensive pipeline; however, they confound the analysis of how best to cluster sequences into OTUs that reflect a specific distance threshold. Through the use of objective criteria that measure the quality of the clusterings, independent of taxonomy or database, researchers will be able to evaluate which clustering algorithm is the best fit for their data.

\newpage

**Figure 1. Comparison of OTU quality generated by multiple algorithms applied to four datasets.** The nearest, average, and furthest neighbor clustering algorithms were used as implemented in mothur (v.1.37). Abundance (AGC) and Distance-based greedy clustering (DGC) were implemented using USEARCH (v.6.1) and VSEARCH (v.1.5.0). Other *de novo* clustering algorithms included Swarm (v.2.1.1), OTUCLUST (v.0.1), and SUMACLUST (v.1.0.20). The MCC values for Swarm were determined by selecting the distance threshold that generated the maximum MCC value for each dataset. The USEARCH and SortMeRNA (v.2.0) closed-reference clusterings were performed using QIIME (v.1.9.1). Closed-reference clustering was also performed using VSEARCH (v.1.5.0) and NINJA-OPS (v.1.3.2). The order of the sequences in each dataset was randomized thirty times and the intra-method range in MCC values was smaller than the plotting symbol. MCC values were calculated using mothur.

\newpage

## References
